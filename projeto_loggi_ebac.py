# -*- coding: utf-8 -*-
"""Projeto_Loggi_EBAC (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/199DryUIj9npauZfry61_ACUwRXRDflpa

<img src="https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png" alt="ebac-logo">

---

# **Módulo** | Análise de Dados: Análise Exploratória de Dados de Logística II
Caderno de **Exercícios**<br>
Professor [André Perez](https://www.linkedin.com/in/andremarcosperez/)

---

# **Tópicos**

<ol type="1">
  <li>Manipulação;</li>
  <li>Visualização;</li>
  <li>Storytelling.</li>
</ol>

---

# **Exercícios**

Este *notebook* deve servir como um guia para **você continuar** a construção da sua própria análise exploratória de dados. Fique a vontate para copiar os códigos da aula mas busque explorar os dados ao máximo. Por fim, publique seu *notebook* no [Kaggle](https://www.kaggle.com/).

---

# **Análise Exploratória de Dados de Logística**

## 1\. Pacotes e bibliotecas
"""

import json
import numpy as np
import pandas as pd
import seaborn as sns
import geopy
import folium
import matplotlib.pyplot as plt
from folium.plugins import MarkerCluster
import plotly.express as px
import random

"""## 2\. Contexto

Este projeto foi desenvolvido para a empresa Loggi, uma das principais empresas de logística do Brasil. O objetivo do projeto foi analisar dados de entregas realizadas pela empresa na região do Distrito Federal, a fim de identificar padrões e tendências que possam ser úteis na otimização do processo logístico.

Utilizamos grafico e mapas para melhor visualização das informações de maneira que permita uma analise mais clara e intuitiva. Baseado nisso a tomada de decisão por parte da empresa se torna mais precisas, gerando uma eficiencia e possivel redução de custos.

## 3\. Exploração de dados

**Coleta de dados**
"""

# Fonte dos dados
!wget -q "https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/deliveries.json" -O deliveries.json

# Coletando os dados
with open('deliveries.json', mode='r', encoding='utf8') as file:
  data = json.load(file)

"""# **Wrangling da estrutura**"""

# Verificando as chaves do arquivo

example = data[0]
print(example.keys())

# Criando o DataFrame

deliveries_df = pd.DataFrame(data)
deliveries_df.head()

# Conversão de dados nested para flatten (Coluna "origin"), criando um novo dataframe para normalizar os dados.

hub_origin_df = pd.json_normalize(deliveries_df["origin"])
hub_origin_df.head()

# Juntando os dois dataframes após normalização dos dados.
deliveries_df = pd.merge(left=deliveries_df, right=hub_origin_df, how='inner', left_index=True, right_index=True)
deliveries_df.head()

# Reorganizando as chaves e renomeando.
deliveries_df = deliveries_df.drop("origin", axis=1)
deliveries_df = deliveries_df[["name", "region", "lng", "lat", "vehicle_capacity", "deliveries"]]
deliveries_df.head()

deliveries_df.rename(columns={"lng": "hub_lng", "lat": "hub_lat"}, inplace=True)
deliveries_df.head()

# Normalizando dados nested da coluna "deliveries" utilizando o método explode e em seguinda os flatten.

deliveries_exploded_df = deliveries_df[["deliveries"]].explode("deliveries")
deliveries_exploded_df.head()

deliveries_normalized_df = pd.concat([
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["size"])).rename(columns={"deliveries": "delivery_size"}),
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["point"]["lng"])).rename(columns={"deliveries": "delivery_lng"}),
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["point"]["lat"])).rename(columns={"deliveries": "delivery_lat"}),
], axis= 1)
deliveries_normalized_df.head()

# Combinando os dados explodidos com o conjunto principal.

deliveries_df = deliveries_df.drop("deliveries", axis=1)
deliveries_df = pd.merge(left=deliveries_df, right=deliveries_normalized_df, how='right', left_index=True, right_index=True)
deliveries_df.reset_index(inplace=True, drop=True)
deliveries_df.head()

# Verificando a estrutura resultante

deliveries_df.shape

deliveries_df.columns

deliveries_df.info()

"""## 4\. Manipulação"""

hub_df = deliveries_df[["region", "hub_lng", "hub_lat"]]
hub_df = hub_df.drop_duplicates().sort_values(by="region").reset_index(drop=True)
hub_df.head()

"""# **Verificando distribuição de entregas por Hub**"""

# Agrupar as entregas por hub e região
grouped_df = deliveries_df.groupby(['name', 'region']).agg({'delivery_size': 'sum', 'vehicle_capacity': 'mean', 'name': 'count'})
grouped_df.rename(columns={'delivery_size': 'total_delivery_size', 'name': 'name_count'}, inplace=True)

# Adicionar a coluna "region"
grouped_df = grouped_df.reset_index()
grouped_df = grouped_df.merge(deliveries_df[['name', 'region']].drop_duplicates(), on='name', how='left')

grouped_df.head()

# Criar um novo dataframe com o número de entregas por região
deliveries_by_region = grouped_df.groupby('region_x').agg({'name_count': 'sum'})

# Ordenar as regiões por número total de entregas
deliveries_by_region = deliveries_by_region.sort_values(by='name_count', ascending=False)

# Criar gráfico de pizza
deliveries_by_region.plot(kind='pie', y='name_count', figsize=(6,6), autopct='%1.1f%%')

# Configurar o gráfico
plt.title("Proporção de Entregas por Região")
plt.ylabel("")
plt.show()

"""## 5\. Visualização

# **Mapa amostral de entregas dividido por Hub**
"""

# Para melhor visualização, trabalharemos com uma amostra de 25 mil entregas.
sample_size = min(25000, len(deliveries_df))
deliveries_df_sample = deliveries_df.sample(n=sample_size, random_state=42)

# Montando o mapa
mapa = px.scatter_mapbox(
    deliveries_df_sample,
    lat="delivery_lat",
    lon="delivery_lng",
    color="region",
    hover_name="region",
    height=600,
    width=800,
    opacity=0.7,
    zoom=8,
)

mapa.update_layout(mapbox_style="open-street-map")
mapa.show()

"""# **Mapa amostral com quantidades de entregas por região.**"""

# Limitar a amostra ao tamanho máximo de 25000 pontos
sample_size = min(25000, len(deliveries_df))

# Realizar a amostragem aleatória do DataFrame para reduzir a quantidade de pontos
deliveries_df_sample = deliveries_df.sample(n=sample_size, random_state=10)

# Criar o mapa centrado na latitude e longitude média dos hubs
mapa = folium.Map(location=[hub_df["hub_lat"].mean(), hub_df["hub_lng"].mean()], zoom_start=10, width='50%', height='50%')

# Adicionar marcadores dos hubs ao mapa a partir do DataFrame hub_df
hub_df.apply(lambda row: folium.Marker([row['hub_lat'], row['hub_lng']], popup=f'Região: {row["region"]}').add_to(mapa), axis=1)

# Agrupando as entregas
marker_cluster = MarkerCluster().add_to(mapa)

# Adicionar pontos das entregas a partir do DataFrame deliveries_df_sample
deliveries_df_sample.apply(lambda row: folium.CircleMarker([row['delivery_lat'], row['delivery_lng']], radius=5, color='blue', fill=True, fill_color='blue', fill_opacity=0.6, popup=f'Região: {row["region"]}').add_to(marker_cluster), axis=1)
mapa